{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99546,"databundleVersionId":11895149,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nimport regex as re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.metrics import classification_report, f1_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom scipy.stats import randint as sp_randint, uniform as sp_uniform\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, LabelEncoder,FunctionTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport category_encoders as ce\n\n\n# # Define column types for preprocessing\n# categorical_columns = ['browser', 'trafficSource.keyword', 'os', 'geoCluster', \n#                        'trafficSource', 'trafficSource.medium', 'trafficSource.referralPath',\n#                        'deviceType', 'userChannel',  'geoNetwork.continent', \n#                        'geoNetwork.subContinent', 'locationCountry']\n# numerical_columns = ['sessionId','sessionNumber', 'pageViews', 'totalHits', 'sessionStart', 'userId','gclIdPresent']\n# boolean_columns = ['device.isMobile']\n\n# # Create preprocessing pipelines for different feature types\n# categorical_pipeline = Pipeline([\n#     ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values with most frequent\n#     ('encoder', OneHotEncoder(handle_unknown='ignore'))    # One-Hot Encoding for categorical features\n# ])\n\n# numerical_pipeline = Pipeline([\n#     ('imputer', SimpleImputer(strategy='median')),  # Handle missing values by replacing with median\n#     ('scaler', StandardScaler())  # Scale the numerical features\n# ])\n\n# boolean_pipeline = Pipeline([\n#     ('encoder', OrdinalEncoder())  # Use OrdinalEncoder for boolean features\n# ])\n\n# # Combine all pipelines into one ColumnTransformer\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('categorical', categorical_pipeline, categorical_columns),\n#         ('numerical', numerical_pipeline, numerical_columns),\n#         ('boolean', boolean_pipeline, boolean_columns)\n#     ])\n\n# Define the model for regression (XGBRegressor) with best hyperparameters\n# model = XGBRegressor(\n#     use_label_encoder=False, \n#     eval_metric='logloss', \n#     random_state=42, \n#     n_jobs=-1,\n#     n_estimators=766,\n#     max_depth=12,\n#     learning_rate=0.04,\n#     subsample=0.8823,\n#     colsample_bytree=0.7021,\n#     gamma=0.2306,\n#     min_child_weight=3,\n#     reg_alpha=0.4916,\n#     reg_lambda=1.2001,\n#     scale_pos_weight=5\n# )\n\n# # Create the full pipeline including preprocessing and modeling\n# full_pipeline = Pipeline([\n#     ('preprocessing', preprocessor),\n#     ('model', model)\n# ])\n\n# # Prepare the training data\n# X_train = train.drop('purchaseValue', axis=1)\n# y_train = train['purchaseValue']\n\n# # Train the model\n# full_pipeline.fit(X_train, y_train)\n\n# # Prepare validation data\n# X = train_df.drop(columns=['purchaseValue'])\n# y = train_df['purchaseValue']\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Evaluate the model's performance on the validation data\n# score = r2_score(y_val, full_pipeline.predict(X_val))\n# print(\"R-squared score on validation data:\", score)\n\n# # Calculate the Mean Squared Error (MSE) for further evaluation\n# mse = mean_squared_error(y_val, full_pipeline.predict(X_val))\n# print(\"Mean Squared Error on validation data:\", mse)\n\n# # Make predictions on the test data\n# X_test = test  # Test dataset without the target variable\n# test_preds = full_pipeline.predict(X_test)\n# test_preds = np.clip(test_preds, 0, None)\n\n# # Prepare the submission DataFrame\n# submission = pd.DataFrame({\n#     'id': subm_df['ID'],  # ID from the sample submission file\n#     'purchaseValue': test_preds  \n# })\n\n# submission.to_csv(\"submission.csv\", index=False)\n\n# print(\"Submission file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:20.547111Z","iopub.execute_input":"2025-07-25T06:52:20.547506Z","iopub.status.idle":"2025-07-25T06:52:28.311862Z","shell.execute_reply.started":"2025-07-25T06:52:20.547473Z","shell.execute_reply":"2025-07-25T06:52:28.311166Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\ntest_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\nsubm_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:53:31.198254Z","iopub.execute_input":"2025-07-25T06:53:31.198632Z","iopub.status.idle":"2025-07-25T06:53:33.146073Z","shell.execute_reply.started":"2025-07-25T06:53:31.198602Z","shell.execute_reply":"2025-07-25T06:53:33.145248Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# train = train_df.loc[:, train_df.nunique() != 1]\n# test = test_df.loc[:, test_df.nunique() != 1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train = train.drop_duplicates()\n\n# train_clean = train.loc[:, train.isnull().mean() < 0.5]\n# test_clean = test.loc[:, test.isnull().mean() < 0.5]\n\n# # I m Dropping unnecessary column\n# cols_to_drop = [ 'trafficSource.campaign', 'geoNetwork.networkDomain', 'geoNetwork.region',\n#                 'geoNetwork.city', 'geoNetwork.metro' ]\n# train.drop(cols_to_drop, axis=1, inplace=True)\n# test.drop(cols_to_drop, axis=1, inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nimport regex as re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.metrics import classification_report, f1_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom scipy.stats import randint as sp_randint, uniform as sp_uniform\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.model_selection import RandomizedSearchCV\nwarnings.filterwarnings('ignore')\n\ntrain_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\ntest_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\nsubm_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv')\n\ntrain = train_df.loc[:, train_df.nunique() != 1]\ntest = test_df.loc[:, test_df.nunique() != 1]\ntrain = train.drop_duplicates()\ntrain_clean = train.loc[:, train.isnull().mean() < 0.5]\ntest_clean = test.loc[:, test.isnull().mean() < 0.5]\n# I m Dropping unnecessary column\ncols_to_drop = [ 'trafficSource.campaign', 'geoNetwork.networkDomain', 'geoNetwork.region',\n                'geoNetwork.city', 'geoNetwork.metro' ]\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)\n\ncategorical_columns = ['browser', 'trafficSource.keyword', 'os', 'geoCluster', \n                       'trafficSource', 'trafficSource.medium', 'trafficSource.referralPath',\n                       'deviceType', 'userChannel',  'geoNetwork.continent', \n                       'geoNetwork.subContinent', 'locationCountry']\nnumerical_columns = ['sessionId','sessionNumber', 'pageViews', 'totalHits', 'sessionStart', 'userId','gclIdPresent']\nboolean_columns = ['device.isMobile']\n\ncategorical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values with most frequent\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))    # One-Hot Encoding for categorical features\n])\nnumerical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values by replacing with median\n    ('scaler', StandardScaler())  # Scale the numerical features\n])\n\nboolean_pipeline = Pipeline([\n    ('encoder', OrdinalEncoder())  # Use OrdinalEncoder for boolean features\n])\n\n# Combine all pipelines into one ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('categorical', categorical_pipeline, categorical_columns),\n        ('numerical', numerical_pipeline, numerical_columns),\n        ('boolean', boolean_pipeline, boolean_columns)\n    ])\n\n\n# --- Prepare features/target ---\nX_train = train.drop('purchaseValue', axis=1)\ny_train = train['purchaseValue']\nX_test = test.copy()\nxgb_param_grid = {\n    'n_estimators': [300, 500, 700, 1000],\n    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n    'max_depth': [6, 8, 10, 12],\n    'subsample': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n    'reg_alpha': [0, 0.1, 0.5, 1],\n    'reg_lambda': [0, 0.1, 0.5, 1]\n}\n\nxgb = XGBRegressor(random_state=42, tree_method='hist', n_jobs=-1, use_label_encoder=False, eval_metric='rmse')\n\n# --- Full pipeline: preprocessing + model ---\nfull_pipeline = Pipeline([\n    ('preprocessing', preprocessor),\n    ('regressor', xgb)\n])\n\n# --- Run RandomizedSearchCV with standard parameters ---\ntuned = RandomizedSearchCV(\n    estimator=full_pipeline,\n    param_distributions={\n        'regressor__' + key: val for key, val in xgb_param_grid.items()\n    },\n    n_iter=20,\n    scoring='r2',\n    cv=5,\n    verbose=2,\n    n_jobs=-1,\n    random_state=42,\n    refit=True\n)\ntuned.fit(X_train, y_train)\nprint(\"Best Parameters found:\", tuned.best_params_)\n\n# --- Predict on test set ---\ntest_preds = tuned.predict(X_test)\ntest_preds = np.clip(test_preds, 0, None)\n\n# --- Prepare submission ---\nsubmission = subm_df.copy()\nsubmission['purchaseValue'] = test_preds\nif 'ID' in submission.columns:\n    submission.rename(columns={'ID': 'id'}, inplace=True)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created: submission_xgb_tuned_standard.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:01:26.187405Z","iopub.execute_input":"2025-07-25T03:01:26.187737Z","iopub.status.idle":"2025-07-25T03:17:33.312661Z","shell.execute_reply.started":"2025-07-25T03:01:26.187716Z","shell.execute_reply":"2025-07-25T03:17:33.311729Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 20 candidates, totalling 100 fits\nBest Parameters found: {'regressor__subsample': 0.7, 'regressor__reg_lambda': 1, 'regressor__reg_alpha': 1, 'regressor__n_estimators': 700, 'regressor__max_depth': 10, 'regressor__learning_rate': 0.1, 'regressor__colsample_bytree': 0.9}\nSubmission file created: submission_xgb_tuned_standard.csv\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  14.7s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  36.8s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.4min\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=300, regressor__reg_alpha=0, regressor__reg_lambda=1, regressor__subsample=0.9; total time=  14.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=300, regressor__reg_alpha=0, regressor__reg_lambda=1, regressor__subsample=0.9; total time=  15.0s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=300, regressor__reg_alpha=0, regressor__reg_lambda=1, regressor__subsample=0.9; total time=  14.8s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  15.0s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  13.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  14.0s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.6min\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=300, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=  10.0s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=300, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=   9.3s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  50.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  53.6s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  51.8s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  29.0s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  23.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  26.3s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  55.1s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  14.1s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=  30.0s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=  29.5s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.05, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.3min\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.05, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.5min\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.01, regressor__max_depth=6, regressor__n_estimators=700, regressor__reg_alpha=0, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  20.4s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.01, regressor__max_depth=6, regressor__n_estimators=700, regressor__reg_alpha=0, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  20.7s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  14.9s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  14.7s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  38.4s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.3min\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=300, regressor__reg_alpha=0, regressor__reg_lambda=1, regressor__subsample=0.9; total time=  14.6s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=300, regressor__reg_alpha=0, regressor__reg_lambda=1, regressor__subsample=0.9; total time=  15.2s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  14.8s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  15.3s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  13.0s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.6min\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=300, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=   9.7s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=300, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=  10.4s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  50.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  54.5s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  49.0s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  29.0s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  29.1s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  25.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  52.5s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  14.3s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  15.3s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=  28.9s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  14.9s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  13.7s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.05, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.4min\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.1min\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.01, regressor__max_depth=6, regressor__n_estimators=700, regressor__reg_alpha=0, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  21.1s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.01, regressor__max_depth=6, regressor__n_estimators=700, regressor__reg_alpha=0, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  20.0s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.01, regressor__max_depth=6, regressor__n_estimators=700, regressor__reg_alpha=0, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  17.0s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  14.7s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  37.0s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.4min\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.4min\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  14.4s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.6min\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=300, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=   9.7s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  52.6s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.5, regressor__subsample=0.8; total time=  14.8s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.5, regressor__subsample=0.8; total time=  13.7s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.5, regressor__subsample=0.8; total time=  14.8s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  50.4s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  29.6s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  30.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  24.5s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  53.2s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  14.2s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  14.5s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=  29.0s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  14.4s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  14.1s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.05, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.4min\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.1min\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.1min\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=0.9; total time=  14.8s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  35.3s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  36.7s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.5min\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  14.6s\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  15.8s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time=  13.5s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.5min\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=1, regressor__reg_lambda=1, regressor__subsample=0.7; total time= 1.5min\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.5, regressor__subsample=0.8; total time=  13.9s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.05, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.5, regressor__subsample=0.8; total time=  14.4s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  48.4s\n[CV] END regressor__colsample_bytree=0.7, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0.1, regressor__reg_lambda=0, regressor__subsample=0.9; total time=  52.1s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.01, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  24.9s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  52.2s\n[CV] END regressor__colsample_bytree=0.8, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__n_estimators=1000, regressor__reg_alpha=0, regressor__reg_lambda=0.5, regressor__subsample=0.9; total time=  54.0s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__n_estimators=500, regressor__reg_alpha=1, regressor__reg_lambda=0.1, regressor__subsample=0.7; total time=  29.6s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0, regressor__subsample=1.0; total time=  13.6s\n[CV] END regressor__colsample_bytree=0.9, regressor__learning_rate=0.05, regressor__max_depth=12, regressor__n_estimators=500, regressor__reg_alpha=0.5, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.4min\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.2min\n[CV] END regressor__colsample_bytree=1.0, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__n_estimators=700, regressor__reg_alpha=0.1, regressor__reg_lambda=0.1, regressor__subsample=1.0; total time= 1.0min\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"thresh = 0.9\nna_cols = train_df.columns[train_df.isnull().mean() > thresh]\n\nprint(na_cols)          # prints the Index of columns exceeding threshold\nprint(list(na_cols))    # prints the column names as a list of strings\n\n# Iterate and print one by one\nfor col in na_cols:\n    print(col)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T05:16:40.510349Z","iopub.execute_input":"2025-07-25T05:16:40.510679Z","iopub.status.idle":"2025-07-25T05:16:40.732489Z","shell.execute_reply.started":"2025-07-25T05:16:40.510648Z","shell.execute_reply":"2025-07-25T05:16:40.731367Z"}},"outputs":[{"name":"stdout","text":"Index(['trafficSource.adContent', 'trafficSource.adwordsClickInfo.slot',\n       'trafficSource.adwordsClickInfo.isVideoAd',\n       'trafficSource.adwordsClickInfo.adNetworkType',\n       'trafficSource.adwordsClickInfo.page'],\n      dtype='object')\n['trafficSource.adContent', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.page']\ntrafficSource.adContent\ntrafficSource.adwordsClickInfo.slot\ntrafficSource.adwordsClickInfo.isVideoAd\ntrafficSource.adwordsClickInfo.adNetworkType\ntrafficSource.adwordsClickInfo.page\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train = train_df.loc[:, train_df.nunique() != 1].drop_duplicates()  # Remove constant columns and duplicates\ntest = test_df.loc[:, test_df.nunique() != 1]\n\n# Drop columns with >50% missing values (can adjust threshold)\nthreshold_na = 0.5\ntrain = train.loc[:, train.isnull().mean() < threshold_na]\ntest = test.loc[:, test.isnull().mean() < threshold_na]\n\n# Friend's manual drop of irrelevant columns\ncols_to_drop = [\n    'trafficSource.campaign', 'geoNetwork.networkDomain', 'geoNetwork.region',\n    'geoNetwork.city', 'geoNetwork.metro'\n]\ntrain = train.drop(cols_to_drop, axis=1, errors='ignore')\ntest = test.drop(cols_to_drop, axis=1, errors='ignore')\n\n# === 3. Missing Value Imputation ===\ncategorical_cols = train.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols = train.select_dtypes(include=[np.number]).columns.tolist()\nif 'purchaseValue' in numerical_cols:\n    numerical_cols.remove('purchaseValue')\n\n# Student's missing value fill:\nfor col in categorical_cols:\n    train[col] = train[col].fillna('missing')\n    test[col] = test[col].fillna('missing')\n\nfor col in numerical_cols:\n    # Use friend’s median imputation for continuous-like columns, 0 for counts/flags below\n    if col in ['pageViews', 'totalHits', 'sessionNumber', 'sessionId']:  # count-like features\n        train[col] = train[col].fillna(0)\n        test[col] = test[col].fillna(0)\n    else:\n        med = train[col].median()\n        train[col] = train[col].fillna(med)\n        test[col] = test[col].fillna(med)\n\n# === 4. Feature Engineering (student’s additions) ===\ndef feature_engineering(df):\n    df = df.copy()\n    # Date features if available\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n        df['dayofweek'] = df['date'].dt.dayofweek\n        df['month'] = df['date'].dt.month\n        df['day'] = df['date'].dt.day\n        df = df.drop(columns=['date'])  # Drop original datetime column here!\n    \n    # Logs and ratios\n    if 'pageViews' in df.columns:\n        df['pageViews_log'] = np.log1p(df['pageViews'])\n    if 'totalHits' in df.columns:\n        df['totalHits_log'] = np.log1p(df['totalHits'])\n    if 'pageViews' in df.columns and 'totalHits' in df.columns:\n        df['hits_per_page'] = df['totalHits'] / (df['pageViews'] + 1)\n        df['page_hit_ratio'] = df['pageViews'] / (df['totalHits'] + 1)\n    \n    # Session-based flags\n    if 'sessionNumber' in df.columns:\n        df['is_returning_user'] = (df['sessionNumber'] > 1).astype(int)\n        df['session_log'] = np.log1p(df['sessionNumber'])\n    \n    return df\n\n\ntrain = feature_engineering(train)\ntest = feature_engineering(test)\n\n# Update categorical and numerical lists after feature engineering\ncategorical_cols = train.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols = train.select_dtypes(include=[np.number]).columns.tolist()\nif 'purchaseValue' in numerical_cols:\n    numerical_cols.remove('purchaseValue')\n\n# === 5. Encoding: LabelEncoder on low cardinality, TargetEncoder on high cardinality ===\n# Separate categorical columns by cardinality\nlow_cardinality_cols = [col for col in categorical_cols if train[col].nunique() <= 10]\nhigh_cardinality_cols = list(set(categorical_cols) - set(low_cardinality_cols))\n\n# Encode low cardinality categoricals with LabelEncoder\nfor col in low_cardinality_cols:\n    le = LabelEncoder()\n    combined_data = pd.concat([train[col], test[col]], axis=0).astype(str)\n    le.fit(combined_data)\n    train[col] = le.transform(train[col].astype(str))\n    test[col]  = le.transform(test[col].astype(str))\n\n# Target encode high cardinality categoricals\nif high_cardinality_cols:\n    target_encoder = ce.TargetEncoder(cols=high_cardinality_cols)\n    train[high_cardinality_cols] = target_encoder.fit_transform(train[high_cardinality_cols], train['purchaseValue'])\n    test[high_cardinality_cols] = target_encoder.transform(test[high_cardinality_cols])\n\n# === 6. Correlation-based Feature Filtering ===\ncorrelations = train.drop('purchaseValue', axis=1).corrwith(train['purchaseValue']).abs()\nlow_corr_features = correlations[correlations < 0.01].index.tolist()\nprint(f\"Dropping {len(low_corr_features)} low-correlation features: {low_corr_features[:10]} ...\")\n\ntrain = train.drop(low_corr_features, axis=1)\ntest = test.drop(low_corr_features, axis=1)\n\n# === 7. Prepare feature matrices for model tuning ===\nX_train = train.drop('purchaseValue', axis=1)\ny_train = train['purchaseValue']\nX_test = test.copy()\n\n# === 8. Build preprocessing pipeline for numeric and boolean features ===\n# Note: categoricals already encoded\n\n# Numerical pipeline: median impute again (safe) + standard scale\nnumerical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Boolean columns (exists in your friend’s pipeline)\nboolean_cols = ['device.isMobile']\nbool_to_int = FunctionTransformer(lambda x: x.astype(int))\n\nboolean_pipeline = Pipeline([\n    ('bool_to_int', bool_to_int),\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder())\n])\n\n# Build the ColumnTransformer for numeric + boolean columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_pipeline, [col for col in numerical_cols if col in X_train.columns]),\n        ('bool', boolean_pipeline, [col for col in boolean_cols if col in X_train.columns])\n    ],\n    remainder='passthrough'  # Pass pre-encoded categorical columns untouched\n)\n\n# === 9. Create full pipeline including XGBoost (to be tuned) ===\nxgb = XGBRegressor(\n    random_state=42,\n    tree_method='hist',\n    n_jobs=-1,\n    use_label_encoder=False,\n    eval_metric='rmse'\n)\n\nfull_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', xgb)\n])\n\n# === 10. Define XGBoost hyperparameter grid for standard tuning ===\nparam_grid = {\n    'regressor__n_estimators': [300, 500, 700, 1000],\n    'regressor__learning_rate': [0.01, 0.03, 0.05, 0.1],\n    'regressor__max_depth': [6, 8, 10, 12],\n    'regressor__subsample': [0.7, 0.8, 0.9, 1.0],\n    'regressor__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n    'regressor__reg_alpha': [0, 0.1, 0.5, 1],\n    'regressor__reg_lambda': [0, 0.1, 0.5, 1]\n}\n\n# === 11. Setup RandomizedSearchCV for tuning ===\ntuned_xgb = RandomizedSearchCV(\n    estimator=full_pipeline,\n    param_distributions=param_grid,\n    n_iter=20,\n    scoring='r2',\n    cv=5,\n    random_state=42,\n    verbose=2,\n    n_jobs=-1,\n    refit=True\n)\n\n# === 12. Fit the tuned pipeline ===\ntuned_xgb.fit(X_train, y_train)\nprint(\"Best hyperparameters found:\", tuned_xgb.best_params_)\n\n# === 13. Predict on test data ===\ntest_predictions = tuned_xgb.predict(X_test)\ntest_predictions = np.clip(test_predictions, 0, None)\n\n# === 14. Prepare submission file ===\nsubmission = subm_df.copy()\nsubmission['purchaseValue'] = test_predictions\n\nif 'ID' in submission.columns:\n    submission.rename(columns={'ID': 'id'}, inplace=True)\n\nsubmission.to_csv('submission_combined_tuned_xgb.csv', index=False)\nprint(\"Submission saved as 'submission_combined_tuned_xgb.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:02:57.238295Z","iopub.execute_input":"2025-07-25T07:02:57.238811Z"}},"outputs":[{"name":"stdout","text":"Dropping 3 low-correlation features: ['geoCluster', 'userId', 'gclIdPresent'] ...\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n","output_type":"stream"}],"execution_count":null}]}